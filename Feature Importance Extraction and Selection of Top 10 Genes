# 1.  Accuracy & Model

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import joblib

from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, accuracy_score
from sklearn.utils.class_weight import compute_class_weight

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras import backend as K

from xgboost import XGBClassifier

# ðŸ“ Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ
output_dir = "/content/drive/MyDrive/GXB_MLP_Improved"
os.makedirs(output_dir, exist_ok=True)

# ðŸ“„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡
df = pd.read_csv(f"{output_dir}/data_top80-200f.csv")
X = df.drop(columns=["Label"]).values
y = (df["Label"] == "progressive").astype(int).values
feature_names = df.drop(columns=["Label"]).columns

# âš–ï¸ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ðŸŽ¯ focal loss
def focal_loss(gamma=2., alpha=0.25):
    def loss(y_true, y_pred):
        y_pred = K.clip(y_pred, K.epsilon(), 1. - K.epsilon())
        cross_entropy = -y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred)
        weight = alpha * K.pow(1 - y_pred, gamma) * y_true + \
                 (1 - alpha) * K.pow(y_pred, gamma) * (1 - y_true)
        return K.mean(weight * cross_entropy)
    return loss

# ØªØ§Ø¨Ø¹ Ø¨Ù‡ØªØ±ÛŒÙ† Ø¢Ø³ØªØ§Ù†Ù‡
def best_threshold(y_true, y_prob):
    thresholds = np.arange(0.1, 0.9, 0.01)
    accs = [accuracy_score(y_true, y_prob > t) for t in thresholds]
    best_idx = np.argmax(accs)
    return thresholds[best_idx], accs[best_idx]

# ðŸ” Cross-validation
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
mlp_auc, mlp_acc, xgb_auc, xgb_acc, combined_accs = [], [], [], [], []
best_auc_mlp, best_auc_xgb = 0, 0
best_model_mlp, best_model_xgb = None, None

for fold, (train_idx, test_idx) in enumerate(kf.split(X_scaled, y), 1):
    print(f"\nðŸ” Fold {fold}")
    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    # âš–ï¸ ÙˆØ²Ù† Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§
    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
    class_weight_dict = dict(enumerate(class_weights))

    # âœ… Ù…Ø¯Ù„ MLP Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡
    K.clear_session()
    model = Sequential([
        Input(shape=(X.shape[1],)),
        Dense(512, activation='relu'),
        BatchNormalization(),
        Dropout(0.3),
        Dense(256, activation='relu'),
        BatchNormalization(),
        Dropout(0.3),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(0.2),
        Dense(64, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer=Adam(learning_rate=0.001), loss=focal_loss(), metrics=['accuracy'])

    es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=0)
    lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)

    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2,
              verbose=0, callbacks=[es, lr_reducer], class_weight=class_weight_dict)

    # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ MLP
    y_prob_mlp = model.predict(X_test).ravel()
    best_thresh, best_acc = best_threshold(y_test, y_prob_mlp)
    y_pred_mlp = (y_prob_mlp > best_thresh).astype(int)

    auc_mlp = roc_auc_score(y_test, y_prob_mlp)
    mlp_auc.append(auc_mlp)
    mlp_acc.append(best_acc)

    if auc_mlp > best_auc_mlp:
        best_auc_mlp = auc_mlp
        best_model_mlp = model

    # âœ… XGBoost Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡
    pos_weight = class_weight_dict[1] / class_weight_dict[0]
    xgb = XGBClassifier(n_estimators=300, max_depth=6, learning_rate=0.03,
                        scale_pos_weight=pos_weight, subsample=0.8, colsample_bytree=0.8,
                        use_label_encoder=False, eval_metric='logloss')
    xgb.fit(X_train, y_train)
    y_prob_xgb = xgb.predict_proba(X_test)[:, 1]
    y_pred_xgb = xgb.predict(X_test)

    auc_xgb = roc_auc_score(y_test, y_prob_xgb)
    acc_xgb = accuracy_score(y_test, y_pred_xgb)
    xgb_auc.append(auc_xgb)
    xgb_acc.append(acc_xgb)

    if auc_xgb > best_auc_xgb:
        best_auc_xgb = auc_xgb
        best_model_xgb = xgb

    # âœ… ØªØ±Ú©ÛŒØ¨ Ù…Ø¯Ù„â€ŒÙ‡Ø§ (Ensemble)
    combined_prob = 0.6 * y_prob_mlp + 0.4 * y_prob_xgb
    combined_pred = (combined_prob > best_thresh).astype(int)
    combined_acc = accuracy_score(y_test, combined_pred)
    combined_accs.append(combined_acc)
    print(f"âœ… Combined Accuracy: {combined_acc:.4f}")

# ðŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§
best_model_mlp.save(f"{output_dir}/best_mlp_model.h5")
joblib.dump(best_model_xgb, f"{output_dir}/best_xgb_model.pkl")
joblib.dump(scaler, f"{output_dir}/scaler.pkl")

# ðŸ“Š Ù†ØªØ§ÛŒØ¬
results_df = pd.DataFrame({
    "Fold": list(range(1, 6)),
    "MLP AUC": mlp_auc,
    "MLP Accuracy": mlp_acc,
    "XGBoost AUC": xgb_auc,
    "XGBoost Accuracy": xgb_acc,
    "Combined Accuracy": combined_accs
})
results_df.loc["Mean"] = ["Mean",
                          np.mean(mlp_auc),
                          np.mean(mlp_acc),
                          np.mean(xgb_auc),
                          np.mean(xgb_acc),
                          np.mean(combined_accs)]
results_df.to_csv(f"{output_dir}/fold_comparison_improved.csv", index=False)

# ðŸ“ˆ Ù†Ù…ÙˆØ¯Ø§Ø± AUC
plt.figure(figsize=(8, 5))
plt.plot(range(1, 6), mlp_auc, marker='o', label='MLP AUC')
plt.plot(range(1, 6), xgb_auc, marker='s', label='XGBoost AUC')
plt.xlabel("Fold")
plt.ylabel("AUC")
plt.title("AUC per Fold")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig(f"{output_dir}/auc_per_fold_improved.png", dpi=300)
plt.show()
